# OpenClaw Clone â€” Complete Architecture & Workflow Guide

## Table of Contents

- [What Is This?](#what-is-this)
- [Tech Stack](#tech-stack)
- [Entry Point](#entry-point)
- [Configuration Layer](#configuration-layer)
- [The Full Request Flow](#the-full-request-flow)
- [Agent Loop â€” The Core Engine](#agent-loop--the-core-engine)
- [Channels (Gateway Layer)](#channels-gateway-layer)
- [Session Store](#session-store)
- [Context Compaction](#context-compaction)
- [Memory Store](#memory-store)
- [Tool Registry & Tools](#tool-registry--tools)
- [Permissions Manager](#permissions-manager)
- [Agent Router (Multi-Agent)](#agent-router-multi-agent)
- [SOUL (System Prompt)](#soul-system-prompt)
- [Command Queue](#command-queue)
- [Heartbeat Scheduler](#heartbeat-scheduler)
- [Runtime Directory Layout](#runtime-directory-layout)
- [Concrete Example Walkthrough](#concrete-example-walkthrough)

---

## What Is This?

A **persistent AI assistant** built in Python (~1500 LOC) that can:

- Talk to you via **multiple channels** (REPL, HTTP API, Telegram)
- **Execute shell commands** with safety permissions
- **Read/write files** on your machine
- **Remember things** across sessions (long-term memory)
- Run **scheduled background tasks** (heartbeats)
- Route messages to **multiple agents** (Jarvis, Scout)

All powered by LLMs via **Anthropic**, **OpenAI**, or the **Portkey AI gateway** â€” pick your provider, set one env var, and go.

---

## Tech Stack

| Component       | Technology                                                                 |
| --------------- | -------------------------------------------------------------------------- |
| Language         | Python 3.12+                                                               |
| LLM Provider      | **Anthropic** / **OpenAI** / **Portkey Gateway** (auto-detected, all OpenAI-compatible interface) |
| Env Config       | `python-dotenv`                                                            |
| Scheduling       | `schedule` library                                                         |
| HTTP Channel     | Flask (optional extra)                                                     |
| Telegram Channel | `python-telegram-bot` (optional extra)                                     |
| Build System     | Hatchling                                                                  |
| Testing          | pytest + pytest-asyncio                                                    |
| Linting          | Ruff                                                                       |

---

## Entry Point

**File:** `src/openclaw/main.py`

Registered as the `openclaw` CLI command via `pyproject.toml`:

```toml
[project.scripts]
openclaw = "openclaw.main:main"
```

Running `openclaw` (or `uv run openclaw`) calls `main()`, which does three things:

1. **Parses CLI arguments:**
   - `--config <path>` â€” path to a JSON config file (optional)
   - `--channel repl|http|telegram` â€” which channel to start (default: `repl`)

2. **Loads configuration:**
   - With `--config`: loads `AppConfig.from_file(path)` and validates it
   - Without `--config`: creates a default `AppConfig()` (env-var driven)

3. **Launches the selected channel:**
   - `repl` â†’ `run_repl()` â€” interactive terminal
   - `http` â†’ `_start_http(config)` â€” Flask web server
   - `telegram` â†’ `_start_telegram(config)` â€” Telegram bot

```
$ openclaw                          # â†’ REPL mode (default)
$ openclaw --channel http           # â†’ HTTP API on port 5000
$ openclaw --channel telegram       # â†’ Telegram bot (needs TELEGRAM_BOT_TOKEN)
$ openclaw -c config.json --channel http  # â†’ HTTP with custom config
```

---

## Configuration Layer

**File:** `src/openclaw/config.py`

Supports **two modes** of configuration:

### Mode 1: Environment Variables (default)

Reads `.env` from the project root via `python-dotenv`. Module-level constants:

| Constant                    | Env Var              | Default                                                       |
| --------------------------- | -------------------- | ------------------------------------------------------------- |
| `LLM_PROVIDER`              | `LLM_PROVIDER`       | `""` (auto-detected from API keys)                            |
| `ANTHROPIC_API_KEY`         | `ANTHROPIC_API_KEY`  | `""`                                                          |
| `OPENAI_API_KEY`            | `OPENAI_API_KEY`     | `""`                                                          |
| `OPENAI_BASE_URL`           | `OPENAI_BASE_URL`    | `https://api.openai.com/v1`                                   |
| `PORTKEY_API_KEY`           | `PORTKEY_API_KEY`    | `""`                                                          |
| `PORTKEY_BASE_URL`          | `PORTKEY_BASE_URL`   | `https://gateway.ai.cimpress.io/v1`                           |
| `DEFAULT_MODEL`             | `OPENCLAW_MODEL`     | `@Anthropic/eu.anthropic.claude-sonnet-4-5-20250929-v1:0`     |
| `WORKSPACE_DIR`             | `OPENCLAW_WORKSPACE` | `~/.mini-openclaw`                                            |
| `MAX_TOOL_TURNS`            | â€”                    | `20`                                                          |
| `COMPACTION_THRESHOLD_TOKENS` | â€”                  | `100,000`                                                     |

**Provider auto-detection priority:** If `LLM_PROVIDER` is not set, the first API key found wins:
1. `PORTKEY_API_KEY` â†’ Portkey gateway
2. `ANTHROPIC_API_KEY` â†’ Anthropic direct
3. `OPENAI_API_KEY` â†’ OpenAI direct

Derived paths (all under `WORKSPACE_DIR`):
- `SESSIONS_DIR` â†’ `~/.mini-openclaw/sessions`
- `MEMORY_DIR` â†’ `~/.mini-openclaw/memory`
- `APPROVALS_FILE` â†’ `~/.mini-openclaw/exec-approvals.json`
- `SOUL_PATH` â†’ `~/.mini-openclaw/SOUL.md`

### Mode 2: JSON Config File (advanced)

`AppConfig.from_file("config.json")` parses a JSON file with:

```json
{
  "workspace": "~/.mini-openclaw",
  "default_model": "@Anthropic/eu.anthropic.claude-sonnet-4-5-20250929-v1:0",
  "agents": {
    "main": { "name": "Jarvis", "soul_path": "...", "model": "..." },
    "research": { "name": "Scout", "prefix": "/research" }
  },
  "channels": {
    "http": { "enabled": true, "port": 5000 }
  },
  "heartbeats": [
    { "name": "morning", "schedule": "every day at 07:30", "prompt": "..." }
  ],
  "permissions": {
    "safe_commands": ["ls", "git", "python"]
  }
}
```

### Key Function: `get_portkey_client()`

Creates an LLM client instance based on the detected provider. Despite the name (kept for backward compatibility), it supports three providers:

| Provider | What it creates | SDK used |
|----------|----------------|----------|
| **Portkey** | `Portkey(api_key, base_url)` | `portkey-ai` (core dependency) |
| **Anthropic** | `OpenAI(api_key, base_url="https://api.anthropic.com/v1/")` | `openai` (optional extra) |
| **OpenAI** | `OpenAI(api_key, base_url)` | `openai` (optional extra) |

All three return a client with the same `client.chat.completions.create(...)` interface, so the rest of the codebase is provider-agnostic.

The helper `_detect_provider()` determines which provider to use:
1. If `LLM_PROVIDER` env var is set â†’ use that
2. Else auto-detect from whichever API key is present (Portkey > Anthropic > OpenAI)

### Key Function: `ensure_workspace()`

Creates `WORKSPACE_DIR`, `SESSIONS_DIR`, and `MEMORY_DIR` if they don't already exist.

---

## The Full Request Flow

Here's the complete journey of a user message through the system:

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  USER INPUT: "What files are in /tmp?"                          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  STEP 1 â€” CHANNEL (REPL / HTTP / Telegram)                     â”‚
 â”‚  Normalizes input into: (user_text, user_id, channel_name)     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  STEP 2 â€” COMMAND QUEUE                                        â”‚
 â”‚  CommandQueue.lock(session_key)                                 â”‚
 â”‚  Acquires per-session lock to prevent concurrent turns          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  STEP 3 â€” AGENT ROUTER                                         â”‚
 â”‚  AgentRouter.resolve(user_text)                                 â”‚
 â”‚  Checks if message starts with a prefix (e.g. "/research")     â”‚
 â”‚  Returns: (AgentConfig, cleaned_text)                          â”‚
 â”‚  Builds session_key: "{session_prefix}:{channel}:{user_id}"    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  STEP 4 â€” SOUL LOADING                                         â”‚
 â”‚  load_soul(soul_path) â†’ reads SOUL.md markdown                 â”‚
 â”‚  build_system_prompt(soul, workspace, date) â†’ system prompt    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  STEP 5 â€” AGENT LOOP (run_agent_turn)                          â”‚
 â”‚  The core LLM â†” Tool execution cycle                           â”‚
 â”‚  See detailed breakdown below                                   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  RESPONSE returned to channel â†’ displayed to user              â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Agent Loop â€” The Core Engine

**File:** `src/openclaw/agent/loop.py`  
**Function:** `run_agent_turn()`

This is the heart of the system. It manages the LLM â†” tool execution cycle.

### Step-by-Step Breakdown

```
run_agent_turn(client, model, system_prompt, session_key, user_text, ...)
â”‚
â”œâ”€â”€ 1. LOAD HISTORY
â”‚     SessionStore.load(session_key) â†’ list of previous messages from JSONL file
â”‚
â”œâ”€â”€ 2. SANITIZE
â”‚     _sanitize_loaded_messages() â†’ removes orphaned tool-call messages
â”‚     (handles crashes where assistant requested tools but results were never saved)
â”‚
â”œâ”€â”€ 3. CHECK COMPACTION
â”‚     estimate_tokens(messages) â†’ rough count (chars / 4)
â”‚     if > 100,000 tokens â†’ compact() â€” summarize old messages (see Compaction section)
â”‚
â”œâ”€â”€ 4. APPEND USER MESSAGE
â”‚     Add {"role": "user", "content": user_text} to history
â”‚     SessionStore.append() â†’ persist to JSONL file immediately
â”‚
â””â”€â”€ 5. LLM LOOP (up to 20 iterations)
      â”‚
      â”œâ”€â”€ Build API payload:
      â”‚     messages = [{"role": "system", "content": system_prompt}] + conversation_history
      â”‚     tools = tool_registry.get_schemas()  (OpenAI function-calling format)
      â”‚
      â”œâ”€â”€ Call LLM:
      â”‚     client.chat.completions.create(model=..., messages=..., tools=..., max_tokens=4096)
      â”‚
      â”œâ”€â”€ If response has NO tool calls:
      â”‚     â†’ Persist assistant message to SessionStore
      â”‚     â†’ RETURN the text response (loop ends)
      â”‚
      â””â”€â”€ If response HAS tool calls:
            â”‚
            â”œâ”€â”€ For each tool_call in response:
            â”‚     â”œâ”€â”€ Parse: tool_name, tool_arguments (JSON)
            â”‚     â”œâ”€â”€ Execute: tool_registry.execute(name, args) â†’ result string
            â”‚     â”œâ”€â”€ Callback: on_tool_use(name, args, result) â†’ prints to terminal
            â”‚     â””â”€â”€ Build: {"role": "tool", "tool_call_id": "...", "content": result}
            â”‚
            â”œâ”€â”€ ATOMIC PERSIST: save assistant message + all tool results together
            â”‚   (prevents orphaned tool-call messages on crash)
            â”‚
            â””â”€â”€ Continue loop â†’ LLM sees tool results and decides next action
```

### Why Atomic Persistence Matters

If the system crashed between saving the assistant's tool-call message and saving the tool results, the next load would have a broken conversation (assistant asking for tools with no answers). The loop saves both atomically, and `_sanitize_loaded_messages()` cleans up any edge cases on load.

### Exit Conditions

| Condition | What Happens |
|-----------|--------------|
| LLM returns text (no tool calls) | Normal exit â€” return response |
| 20 tool iterations reached | Safety exit â€” returns `"(max tool turns reached)"` |

---

## Channels (Gateway Layer)

All channels implement the `ChannelAdapter` abstract base class:

**File:** `src/openclaw/channels/base.py`

```python
class ChannelAdapter(ABC):
    @property
    def name(self) -> str: ...       # "repl", "http", "telegram"
    def start(self) -> None: ...     # Start the channel
    def stop(self) -> None: ...      # Gracefully stop
```

### REPL Channel

**File:** `src/openclaw/channels/repl.py`

The default interactive terminal interface.

**What `run_repl()` does:**

1. **Setup workspace** â€” creates dirs, copies default `SOUL.md` if missing
2. **Initialize all components:**
   - `get_portkey_client()` â†’ LLM client (Anthropic, OpenAI, or Portkey â€” auto-detected)
   - `SessionStore(SESSIONS_DIR)` â†’ session persistence
   - `MemoryStore(MEMORY_DIR)` â†’ long-term memory
   - `PermissionManager(APPROVALS_FILE, approval_callback=_approval_prompt)` â†’ with interactive y/n prompt
   - `ToolRegistry` â†’ registers all 6 tools
   - `CommandQueue()` â†’ per-session locking
3. **Set up multi-agent router:**
   - Default agent: **Jarvis** (no prefix)
   - Research agent: **Scout** (`/research` prefix)
4. **Enter input loop:**
   - Read user input via `input("You: ")`
   - Handle commands: `/quit`, `/new` (reset session), `/research <query>`
   - Route message through `AgentRouter` â†’ `CommandQueue` â†’ `run_agent_turn()`
   - Print response with agent label

**Special commands:**
- `/new` â€” resets the session by changing the user_id suffix
- `/quit` / `/exit` / `/q` â€” exits the REPL
- `/research <query>` â€” routes to the Scout agent

### HTTP Channel

**File:** `src/openclaw/channels/http_api.py`

Flask-based REST API. Runs in a daemon thread.

**Endpoints:**

| Method | Path        | Description                                    |
| ------ | ----------- | ---------------------------------------------- |
| `POST` | `/chat`     | Send `{"message": "...", "user_id": "..."}` â†’ `{"response": "..."}` |
| `GET`  | `/health`   | Returns `{"status": "ok", "agents": [...]}` |
| `GET`  | `/sessions` | Lists all session filenames |

**Request flow:** `POST /chat` â†’ parse JSON â†’ resolve agent â†’ acquire session lock â†’ run agent turn â†’ return JSON response.

### Telegram Channel

**File:** `src/openclaw/channels/telegram.py`

Uses `python-telegram-bot` library with **long-polling** (no webhook needed).

**Handlers:**
- `/start` â€” welcome message with agent names and available commands
- `/new` â€” reset session (changes session suffix in `user_data`)
- `/research <query>` â€” explicit command handler for the research agent
- Regular text messages â€” routed through the agent router

**Features:**
- Shows "typing..." indicator while the LLM responds
- Splits responses longer than 4096 chars (Telegram's limit) into multiple messages
- Logs all activity to terminal for operator visibility
- Each Telegram chat gets its own isolated session key

---

## Session Store

**File:** `src/openclaw/session/store.py`

Persists conversation history as **JSONL files** (one JSON object per line).

### How It Works

```
Session Key: "agent:main:repl:repl"
     â†“ sanitize
Filename: "agent_main_repl_repl.jsonl"
     â†“
Location: ~/.mini-openclaw/sessions/agent_main_repl_repl.jsonl
```

**File format** (each line is a JSON message):
```jsonl
{"role": "user", "content": "What files are in /tmp?"}
{"role": "assistant", "content": null, "tool_calls": [{"id": "call_123", "type": "function", "function": {"name": "run_command", "arguments": "{\"command\": \"ls /tmp\"}"}}]}
{"role": "tool", "tool_call_id": "call_123", "content": "foo.txt bar.txt"}
{"role": "assistant", "content": "The files in /tmp are: foo.txt and bar.txt"}
```

### Key Methods

| Method           | What It Does                                                       |
| ---------------- | ------------------------------------------------------------------ |
| `load(key)`      | Read all messages from JSONL file. Returns `[]` if file doesn't exist. Skips corrupted lines. |
| `append(key, msg)` | Append one message line (crash-safe â€” append-only write) |
| `save(key, msgs)` | Overwrite entire file (used after compaction) |
| `list_sessions()` | List all session filenames (without `.jsonl` extension) |
| `delete(key)`    | Delete a session file |
| `message_count(key)` | Count messages without loading into memory |

### Crash Safety

- **Append-only writes** â†’ at most one line lost on crash
- Corrupted lines (partial JSON from crash) are silently skipped on load
- Orphaned tool-call messages cleaned up by the agent loop's sanitizer

---

## Context Compaction

**File:** `src/openclaw/session/compaction.py`

Prevents conversations from exceeding the LLM's context window.

### How It Works

```
Messages in session â†’ estimate_tokens() â†’ chars / 4
     â”‚
     â”œâ”€â”€ Under 100K tokens â†’ no-op, return messages as-is
     â”‚
     â””â”€â”€ Over 100K tokens â†’ COMPACT:
           â”‚
           â”œâ”€â”€ Split messages at midpoint (on a user-message boundary)
           â”‚   â”œâ”€â”€ OLD HALF: first ~50% of messages
           â”‚   â””â”€â”€ RECENT HALF: last ~50% of messages
           â”‚
           â”œâ”€â”€ Format OLD HALF into readable text
           â”‚
           â”œâ”€â”€ Ask LLM to summarize:
           â”‚   "Summarize the following conversation concisely.
           â”‚    Preserve all important facts, decisions, user preferences,
           â”‚    file paths, variable names, and action outcomes."
           â”‚
           â”œâ”€â”€ Replace OLD HALF with one summary message:
           â”‚   {"role": "user", "content": "[Conversation summary of N messages]\n\nâ€¢ ..."}
           â”‚
           â””â”€â”€ Return: [summary_message] + RECENT HALF
               (much shorter â€” fits in context window)
```

### Smart Splitting

The split tries to land on a **user-message boundary** to avoid breaking an assistantâ†’toolâ†’result sequence in the middle. It searches forward from midpoint, then backward, then falls back to exact midpoint.

---

## Memory Store

**File:** `src/openclaw/memory/store.py`

Long-term file-based memory that **persists across session resets** and is **shared across all agents**.

### Storage Format

Each memory is a **markdown file**:
```
~/.mini-openclaw/memory/
â”œâ”€â”€ user-preferences.md      â† key: "user-preferences"
â”œâ”€â”€ project-notes.md         â† key: "project-notes"
â””â”€â”€ meeting-summary-jan.md   â† key: "meeting-summary-jan"
```

### Key Methods

| Method          | What It Does                                              |
| --------------- | --------------------------------------------------------- |
| `save(key, content)` | Write/overwrite a markdown memory file |
| `load(key)`     | Read a specific memory by key. Returns `None` if not found. |
| `search(query)` | Keyword search (case-insensitive) across all memory files. Splits query into words, matches any word. |
| `list_keys()`   | List all memory keys (filenames without `.md`) |
| `delete(key)`   | Delete a memory file |

### How the Agent Uses Memory

The agent has two tools that bridge to this store:
- **`save_memory`** â€” the agent calls this when it encounters important information (user preferences, project details, key decisions)
- **`memory_search`** â€” the agent calls this to recall information from previous sessions

The SOUL instructs the agent: *"Use `memory_search` at the start of conversations to recall context from previous sessions."*

---

## Tool Registry & Tools

**File:** `src/openclaw/tools/registry.py`

### Tool Data Model

Each tool is a dataclass:
```python
@dataclass
class Tool:
    name: str                           # Unique ID (e.g. "run_command")
    description: str                    # For the LLM to understand when to use it
    input_schema: dict[str, Any]        # JSON Schema for parameters
    handler: Callable[[dict], str]      # The actual execution function
```

### Registry Operations

| Method                  | What It Does                                                 |
| ----------------------- | ------------------------------------------------------------ |
| `register(tool)`        | Add a tool. Raises `ValueError` on duplicate names. |
| `get_schemas()`         | Returns all tools in **OpenAI function-calling format** for the API |
| `execute(name, input)`  | Dispatches to the tool's handler. Catches exceptions â€” never raises. |
| `tool_names`            | List of registered tool names |

### The 6 Registered Tools

#### 1. `run_command` â€” Shell Execution
**File:** `src/openclaw/tools/shell.py`

```
Input:  {"command": "ls -la /tmp"}
Output: "total 48\ndrwxrwxrwt  12 root  wheel  384 Feb 16 10:00 .\n..."
```

- Runs via `subprocess.run()` with `shell=True`
- **30-second timeout** â€” prevents hanging
- **Permission-checked** â€” goes through `PermissionManager` first
- Captures both stdout and stderr

#### 2. `read_file` â€” File Reading
**File:** `src/openclaw/tools/filesystem.py`

```
Input:  {"path": "/Users/me/project/README.md"}
Output: "# My Project\n\nThis is a readme..."
```

- Max read size: **50,000 characters** (truncates with notice)
- Handles: `FileNotFoundError`, `IsADirectoryError`

#### 3. `write_file` â€” File Writing
**File:** `src/openclaw/tools/filesystem.py`

```
Input:  {"path": "/tmp/hello.txt", "content": "Hello, world!"}
Output: "Wrote 13 characters to /tmp/hello.txt"
```

- Creates parent directories automatically (`os.makedirs`)

#### 4. `save_memory` â€” Long-Term Memory Save
**File:** `src/openclaw/tools/memory_tools.py`

```
Input:  {"key": "user-preferences", "content": "Prefers dark mode. Uses Python 3.12."}
Output: "Saved to memory: user-preferences"
```

#### 5. `memory_search` â€” Long-Term Memory Search
**File:** `src/openclaw/tools/memory_tools.py`

```
Input:  {"query": "preferences"}
Output: "--- user-preferences ---\nPrefers dark mode. Uses Python 3.12."
```

#### 6. `web_search` â€” Web Search (stub)
**File:** `src/openclaw/tools/web.py`

```
Input:  {"query": "Python 3.13 release date"}
Output: "[Web search stub] Results for: Python 3.13 release date\nNote: Connect a real search API..."
```

Currently a placeholder. To enable real search, connect SerpAPI, Brave Search, or similar.

---

## Permissions Manager

**File:** `src/openclaw/permissions/manager.py`

Safety layer for shell command execution. Prevents the AI from running dangerous commands without your explicit approval.

### Decision Flow

```
Command arrives: "rm -rf /important"
     â”‚
     â–¼
PermissionManager.check(command)
     â”‚
     â”œâ”€â”€ Is base command in SAFE SET?
     â”‚   (ls, cat, git, python, grep, find, echo, etc.)
     â”‚   â†’ YES â†’ return "safe" â†’ auto-execute
     â”‚
     â”œâ”€â”€ Was this EXACT command previously approved?
     â”‚   (check exec-approvals.json)
     â”‚   â†’ YES â†’ return "approved" â†’ auto-execute
     â”‚
     â””â”€â”€ Neither?
         â†’ return "needs_approval"
              â”‚
              â–¼
         PermissionManager.request_approval(command)
              â”‚
              â”œâ”€â”€ REPL: prints "âš ï¸ Command: rm -rf /important"
              â”‚          prompts "Allow? (y/n): "
              â”‚
              â”œâ”€â”€ User says YES â†’ save to "allowed" in JSON â†’ execute
              â””â”€â”€ User says NO  â†’ save to "denied" in JSON â†’ return "Permission denied"
```

### Safe Commands (default set)

```
ls, cat, head, tail, wc, date, whoami, echo, pwd, which,
git, python, python3, node, npm, npx, uv, pip, find, grep,
sort, uniq, tr, cut, env
```

### Persistent Approvals

Stored in `~/.mini-openclaw/exec-approvals.json`:
```json
{
  "allowed": ["docker ps", "brew install jq"],
  "denied": ["rm -rf /"]
}
```

Once approved, you're **never asked again** for the same exact command.

---

## Agent Router (Multi-Agent)

**File:** `src/openclaw/agent/router.py`

Routes messages to different agents based on **prefix commands**.

### Registered Agents

| Agent      | Prefix       | Session Prefix   | SOUL File            |
| ---------- | ------------ | ---------------- | -------------------- |
| **Jarvis** | *(none â€” default)* | `agent:main` | `workspace/SOUL.md` |
| **Scout**  | `/research`  | `agent:research` | `SCOUT.md` (if exists, else built-in default) |

### How Routing Works

```python
router.resolve("/research quantum computing")
# â†’ (Scout AgentConfig, "quantum computing")

router.resolve("hello there")
# â†’ (Jarvis AgentConfig, "hello there")
```

1. `resolve(user_text)` â€” checks if text starts with any registered prefix
2. If match â†’ returns that agent config + strips the prefix from the text
3. If no match â†’ returns the default agent (Jarvis) + original text
4. `run()` â€” calls `resolve()`, builds the session key, and calls `run_agent_turn()`

### Session Isolation

Each agent gets its own session key pattern:
- Jarvis: `agent:main:repl:repl`
- Scout: `agent:research:repl:repl`

This means agents have **separate conversation histories** but share the same tools and memory store.

### `AgentConfig` Dataclass

```python
@dataclass
class AgentConfig:
    name: str                    # "Jarvis", "Scout"
    model: str                   # LLM model identifier
    soul_path: str | None        # Path to SOUL.md
    prefix: str | None           # "/research" or None (default agent)
    session_prefix: str          # "agent:main", "agent:research"
    workspace_path: str | None   # Injected into system prompt
```

The `system_prompt` property is **lazily computed** â€” loads the SOUL and builds the full prompt only on first access, then caches it.

---

## SOUL (System Prompt)

**File:** `src/openclaw/agent/soul.py`

The SOUL defines the agent's **personality, behavior, and boundaries**. It's a markdown file that becomes the system prompt.

### Default SOUL (`workspace/SOUL.md`)

```markdown
# Who You Are

**Name:** Jarvis
**Role:** Personal AI assistant

## Personality
- Be genuinely helpful, not performatively helpful
- Skip the "Great question!" â€” just help
- Have opinions. You're allowed to disagree
- Be concise when needed, thorough when it matters

## Boundaries
- Private things stay private
- When in doubt, ask before acting externally

## Memory
- Use save_memory to store important information
- Use memory_search at the start of conversations to recall context
```

### System Prompt Building

`build_system_prompt()` combines three parts:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SOUL text (from SOUL.md)        â”‚
â”‚                                     â”‚
â”‚  2. Dynamic context:                â”‚
â”‚     - Current date: 2026-02-16      â”‚
â”‚     - Workspace: ~/.mini-openclaw   â”‚
â”‚                                     â”‚
â”‚  3. Extra context (if any)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    Full system prompt string
    (sent as the "system" message on every LLM API call)
```

---

## Command Queue

**File:** `src/openclaw/queue/command_queue.py`

Prevents **concurrent agent turns** on the same session. Critical for multi-user channels (HTTP, Telegram).

### How It Works

```python
queue = CommandQueue()

# Session "A" â€” acquires lock immediately
with queue.lock("session:A"):
    run_agent_turn(...)  # Only one turn at a time for session A

# Session "B" â€” different session, runs in parallel
with queue.lock("session:B"):
    run_agent_turn(...)  # No conflict with session A
```

### Implementation

- `defaultdict(threading.Lock)` â€” one lock per session key
- A **meta-lock** protects the dictionary itself (safe get-or-create)
- The meta-lock is released *before* acquiring the session lock (so other sessions aren't blocked)
- `active_sessions` property â€” shows which sessions currently hold locks (monitoring)

---

## Heartbeat Scheduler

**File:** `src/openclaw/heartbeat/scheduler.py`

Runs **recurring agent tasks** without human input â€” like cron for your AI.

### How It Works

```
HeartbeatScheduler
     â”‚
     â”œâ”€â”€ add(Heartbeat("morning", "every day at 07:30",
     â”‚        prompt="What's on my agenda today?"))
     â”‚
     â”œâ”€â”€ start() â†’ spawns daemon thread
     â”‚     â””â”€â”€ Every 30 seconds: scheduler.run_pending()
     â”‚           â””â”€â”€ If a heartbeat is due:
     â”‚                 â”œâ”€â”€ Build session key: "cron:morning"
     â”‚                 â”œâ”€â”€ Call run_fn(agent_name, session_key, prompt)
     â”‚                 â””â”€â”€ Fire on_result callback
     â”‚
     â””â”€â”€ stop() â†’ signals thread to exit
```

### Schedule Expression Parsing

Supports human-readable expressions:

| Expression                | Meaning                      |
| ------------------------- | ---------------------------- |
| `"every 5 minutes"`       | Every 5 minutes              |
| `"every 1 hour"`          | Every hour                   |
| `"every day at 07:30"`    | Daily at 7:30 AM             |
| `"every monday at 09:00"` | Weekly on Monday at 9 AM     |
| `"every 30 seconds"`      | Every 30 seconds (for testing) |

### Session Isolation

Each heartbeat gets its own session key (`cron:<name>`), so scheduled tasks **don't pollute** interactive conversations.

---

## Runtime Directory Layout

After running OpenClaw, your workspace looks like:

```
~/.mini-openclaw/                      â† WORKSPACE_DIR
â”œâ”€â”€ sessions/                          â† Conversation history (JSONL files)
â”‚   â”œâ”€â”€ agent_main_repl_repl.jsonl           â† Jarvis REPL session
â”‚   â”œâ”€â”€ agent_research_repl_repl.jsonl       â† Scout REPL session
â”‚   â”œâ”€â”€ agent_main_http_anonymous.jsonl      â† HTTP API session
â”‚   â”œâ”€â”€ agent_main_telegram_12345_default.jsonl  â† Telegram chat
â”‚   â””â”€â”€ cron_morning.jsonl                   â† Heartbeat session
â”‚
â”œâ”€â”€ memory/                            â† Long-term memory (markdown files)
â”‚   â”œâ”€â”€ user-preferences.md
â”‚   â”œâ”€â”€ project-notes.md
â”‚   â””â”€â”€ meeting-summary-jan.md
â”‚
â”œâ”€â”€ exec-approvals.json                â† Persisted command approvals
â”œâ”€â”€ SOUL.md                            â† Agent personality (Jarvis)
â”œâ”€â”€ SCOUT.md                           â† Research agent personality (optional)
â””â”€â”€ config.json                        â† Optional advanced config
```

---

## Concrete Example Walkthrough

Here's exactly what happens when you type a message in the REPL:

### Input
```
You: What files are in /tmp?
```

### Step-by-Step Execution

```
1. REPL reads "What files are in /tmp?" via input()

2. CommandQueue.lock("agent:main:repl:repl")
   â†’ Acquires per-session lock (instant â€” no contention in REPL)

3. AgentRouter.resolve("What files are in /tmp?")
   â†’ No prefix match â†’ returns (Jarvis config, "What files are in /tmp?")
   â†’ session_key = "agent:main:repl:repl"

4. run_agent_turn() begins:
   a. SessionStore.load("agent:main:repl:repl")
      â†’ Reads agent_main_repl_repl.jsonl â†’ [previous messages]

   b. _sanitize_loaded_messages() â†’ removes any orphaned tool calls

   c. estimate_tokens() â†’ e.g. 2,400 tokens â†’ under threshold, no compaction

   d. Appends {"role": "user", "content": "What files are in /tmp?"}
      â†’ SessionStore.append() â†’ writes to JSONL

   e. LLM LOOP â€” Iteration 1:
      â†’ Calls LLM API (provider-dependent):
        POST <provider_base_url>/chat/completions
        {
          "model": "<configured model e.g. claude-sonnet-4-5-20250929 or gpt-4o>",
          "messages": [
            {"role": "system", "content": "# Who You Are\n**Name:** Jarvis\n..."},
            ...previous messages...,
            {"role": "user", "content": "What files are in /tmp?"}
          ],
          "tools": [...6 tool schemas...],
          "max_tokens": 4096
        }

      â† LLM responds with tool_call:
        run_command({"command": "ls /tmp"})

      â†’ PermissionManager.check("ls /tmp")
        â†’ base command "ls" is in safe set â†’ "safe"

      â†’ subprocess.run("ls /tmp", shell=True, timeout=30)
        â†’ stdout: "foo.txt\nbar.txt\ndata.csv"

      â†’ on_tool_use callback prints:
        ğŸ”§ run_command: {"command": "ls /tmp"}
           â†’ foo.txt\nbar.txt\ndata.csv

      â†’ ATOMIC PERSIST: assistant tool-call msg + tool result msg

   f. LLM LOOP â€” Iteration 2:
      â†’ Calls LLM API again (now with tool result in messages)

      â† LLM responds with text (no tool calls):
        "Here are the files in /tmp:\n- foo.txt\n- bar.txt\n- data.csv"

      â†’ Persist assistant message â†’ loop exits

5. CommandQueue releases the lock

6. REPL prints:
   ğŸ¤– Here are the files in /tmp:
   - foo.txt
   - bar.txt
   - data.csv
```

### What Got Persisted

**Session file** (`~/.mini-openclaw/sessions/agent_main_repl_repl.jsonl`):
```jsonl
{"role": "user", "content": "What files are in /tmp?"}
{"role": "assistant", "content": null, "tool_calls": [{"id": "call_abc", "type": "function", "function": {"name": "run_command", "arguments": "{\"command\": \"ls /tmp\"}"}}]}
{"role": "tool", "tool_call_id": "call_abc", "content": "foo.txt\nbar.txt\ndata.csv"}
{"role": "assistant", "content": "Here are the files in /tmp:\n- foo.txt\n- bar.txt\n- data.csv"}
```

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Gateway                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Telegram  â”‚  â”‚ Discord  â”‚  â”‚ HTTP API â”‚  â”‚  REPL  â”‚  â”‚
â”‚  â”‚ Adapter   â”‚  â”‚ Adapter  â”‚  â”‚ Adapter  â”‚  â”‚Adapter â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                         â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   Command Queue     â”‚                    â”‚
â”‚              â”‚  (per-session lock) â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                         â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   Agent Router      â”‚                    â”‚
â”‚              â”‚  (multi-agent)      â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                         â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   Agent Loop        â”‚                    â”‚
â”‚              â”‚  LLM â†” Tools cycle  â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                         â”‚                               â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚        â–¼                â–¼                â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Sessions â”‚   â”‚    Tools     â”‚  â”‚  Memory  â”‚         â”‚
â”‚  â”‚ (JSONL)  â”‚   â”‚  Registry    â”‚  â”‚  Store   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                         â”‚                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚   Permissions       â”‚                    â”‚
â”‚              â”‚   (allowlist)       â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Heartbeat Scheduler                â”‚    â”‚
â”‚  â”‚  (cron jobs â†’ injects messages into queue)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Source File Map

| File | Lines | Purpose |
|------|-------|---------|
| `src/openclaw/main.py` | ~170 | CLI entry point, channel launching |
| `src/openclaw/config.py` | ~230 | Configuration loading (env + JSON) |
| `src/openclaw/agent/loop.py` | ~170 | Core LLM â†” tool execution loop |
| `src/openclaw/agent/router.py` | ~150 | Multi-agent prefix-based routing |
| `src/openclaw/agent/soul.py` | ~85 | SOUL.md loading & system prompt building |
| `src/openclaw/channels/base.py` | ~30 | Abstract channel interface |
| `src/openclaw/channels/repl.py` | ~150 | Interactive terminal channel |
| `src/openclaw/channels/http_api.py` | ~130 | Flask REST API channel |
| `src/openclaw/channels/telegram.py` | ~240 | Telegram bot channel |
| `src/openclaw/session/store.py` | ~110 | JSONL session persistence |
| `src/openclaw/session/compaction.py` | ~110 | Context window compression |
| `src/openclaw/memory/store.py` | ~100 | Long-term file-based memory |
| `src/openclaw/tools/registry.py` | ~95 | Tool registration & dispatch |
| `src/openclaw/tools/shell.py` | ~55 | Shell command execution tool |
| `src/openclaw/tools/filesystem.py` | ~80 | File read/write tools |
| `src/openclaw/tools/memory_tools.py` | ~65 | Memory save/search tools |
| `src/openclaw/tools/web.py` | ~35 | Web search stub tool |
| `src/openclaw/permissions/manager.py` | ~100 | Command approval & allowlist |
| `src/openclaw/queue/command_queue.py` | ~50 | Per-session concurrency locks |
| `src/openclaw/heartbeat/scheduler.py` | ~190 | Cron-like scheduled agent tasks |
